{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshandugar2004/CorporateMailHandler/blob/main/Bert_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b49a7d3"
      },
      "source": [
        "# Corporate Email Categorization Model\n",
        "\n",
        "This is a fine-tuned DistilBERT model for classifying corporate emails into various categories.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This project focuses on building a robust model for corporate email categorization to automate the sorting and routing of internal and external communications. The process involves standard NLP techniques, data augmentation for handling class imbalance, and fine-tuning a pre-trained transformer model.\n",
        "\n",
        "## Approach and Methodology\n",
        "\n",
        "The fine-tuning process for this model followed these key steps:\n",
        "\n",
        "1.  **Data Loading**: The initial dataset was loaded from the specified source.\n",
        "2.  **Exploratory Data Analysis (EDA)**: Basic EDA was performed to understand the data structure, identify missing values, and analyze the distribution of labels.\n",
        "3.  **Handling Data Imbalance**: It was observed that some categories had significantly fewer examples than others. To address this, data augmentation was employed.\n",
        "4.  **Artificial Data Generation**: Synthetic email examples were generated for under-represented classes using the Groq API and the Llama3-8b-8192 model. This helped to balance the dataset and improve the model's ability to learn from minority classes.\n",
        "5.  **Encoding**: String labels were converted into numerical representations (integer IDs) required for model training.\n",
        "6.  **Tokenization**: The email text data was tokenized using the `distilbert-base-uncased` tokenizer. This process converts the text into a sequence of numerical tokens that the model can process, including adding special tokens and attention masks.\n",
        "7.  **Data Splitting**: The combined original and augmented dataset was split into training, validation, and test sets to facilitate model training and evaluation.\n",
        "8.  **Model Initialization**: A pre-trained `distilbert-base-uncased` model was loaded. For the classification task, output layers were added to the model to predict the defined categories.\n",
        "9.  **Training**: The model was fine-tuned on the augmented training dataset using the Hugging Face `Trainer`. The training involved optimizing the model's parameters to minimize the classification loss.\n",
        "10. **Evaluation**: The fine-tuned model's performance was evaluated on a separate, unseen test set.\n",
        "\n",
        "## Training Details\n",
        "\n",
        "*   **Base Model**: `distilbert-base-uncased`\n",
        "*   **Training Method**: Fine-tuning with added output layers for sequence classification. (While LoRA was mentioned, the provided notebook code does not explicitly show LoRA implementation. This description reflects the standard fine-tuning with classification layers as seen in the code.)\n",
        "*   **Epochs**: 5\n",
        "*   **Batch Size**: 16 (Training and Evaluation)\n",
        "*   **Optimizer**: AdamW\n",
        "*   **Learning Rate Scheduler**: Linear warmup\n",
        "*   **Loss Function**: Cross-Entropy Loss (implicitly used by `AutoModelForSequenceClassification`)\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "The model's performance on the final test set is summarized below:\n",
        "\n",
        "| Metric             | Value    |\n",
        "|--------------------|----------|\n",
        "|accuracy           |  0.9130 |\n",
        "| f1_weighted        |  0.7949 |\n",
        "| precision_weighted |  0.8462 |\n",
        "| recall_weighted    |  0.7692\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMe_ZZULPaP0"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available (highly recommended for faster training)\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU. Training might be slow.\")\n",
        "\n",
        "# 1. Install necessary libraries\n",
        "!pip install -qq transformers datasets accelerate scikit-learn pandas tabulate groq\n",
        "\n",
        "# Imports\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate # Import tabulate for pretty printing\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0yHu190QwO4"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"infinite-dataset-hub/CorporateMailCategorization\")\n",
        "\n",
        "# Convert to pandas DataFrame for easier null handling\n",
        "df = ds[\"train\"].to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsWr7aEEPcqE"
      },
      "outputs": [],
      "source": [
        "initial_rows = len(df)\n",
        "df_cleaned = df.dropna(subset=['label']).copy()\n",
        "rows_dropped = initial_rows - len(df_cleaned)\n",
        "print(f\"\\nDropped {rows_dropped} rows with null 'label' values.\")\n",
        "print(f\"Remaining rows after dropping nulls: {len(df_cleaned)}\")\n",
        "df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckogZ_92kL4b"
      },
      "outputs": [],
      "source": [
        "is_duplicate_counts = df_cleaned.duplicated().value_counts()\n",
        "print(is_duplicate_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY6w6a3BRC3K"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "label_counts = df_cleaned['label'].value_counts()\n",
        "label_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUh7IeAqRA5P"
      },
      "outputs": [],
      "source": [
        "# Important: Map string labels to integers for the model\n",
        "# Get unique labels from the cleaned dataset\n",
        "unique_labels = sorted(df_cleaned['label'].unique().tolist())\n",
        "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "id_to_label = {i: label for i, label in enumerate(unique_labels)}\n",
        "\n",
        "print(f\"\\nDetected labels and their mappings: {label_to_id}\")\n",
        "num_labels = len(unique_labels)\n",
        "print(f\"Number of unique labels: {num_labels}\")\n",
        "\n",
        "# Apply label mapping to the cleaned dataset\n",
        "def map_labels_to_ids(example):\n",
        "    example['label'] = label_to_id[example['label']]\n",
        "    return example\n",
        "\n",
        "# Convert cleaned DataFrame to Hugging Face Dataset\n",
        "full_labeled_ds = Dataset.from_pandas(df_cleaned).map(map_labels_to_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWD5WEUc6kix"
      },
      "outputs": [],
      "source": [
        "id_to_label\n",
        "label_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKxv9bPyhJUZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "class SyntheticTextGenerator:\n",
        "    def __init__(self, id_to_label_map):\n",
        "        self.client = Groq()\n",
        "        self.groq_model = \"llama-3.1-8b-instant\"\n",
        "        self.id_to_label = id_to_label_map # Store the mapping for better prompts\n",
        "\n",
        "    def generate_similar_texts(self, reference_texts: list, num_to_generate: int, target_label_id: int) -> list:\n",
        "        \"\"\"\n",
        "        Generates new text examples similar to reference_texts for a given label,\n",
        "        expecting JSON output from the LLM.\n",
        "        \"\"\"\n",
        "        target_label_name = self.id_to_label.get(target_label_id, \"Unknown Category\")\n",
        "\n",
        "        if not reference_texts:\n",
        "            print(f\"Warning: No reference texts provided for label ID {target_label_id} ({target_label_name}). Cannot generate examples.\")\n",
        "            return []\n",
        "\n",
        "        # Prepare reference examples for the prompt\n",
        "        reference_json_examples = []\n",
        "        for text in reference_texts:\n",
        "            # Ensure reference examples are also clean JSON lines\n",
        "            reference_json_examples.append(json.dumps({\"text\": text, \"label\": target_label_name}))\n",
        "        reference_str = \",\\n\".join(reference_json_examples)\n",
        "\n",
        "        # Adjusted system prompt for more robust JSON output instructions\n",
        "        system_prompt = (\n",
        "            \"You are an AI assistant specialized in generating realistic and diverse corporate email snippets \"\n",
        "            \"for data augmentation. Your output MUST be a strict JSON array where each element is a JSON object with 'text' and 'label' keys. \"\n",
        "            \"It must start with `[` and end with `]`. \"\n",
        "            \"Do NOT include any additional text, explanations, or formatting outside of the JSON array. \"\n",
        "            \"Do NOT wrap the array in any other JSON object, like {\\\"data\\\": [...]}. Provide ONLY the JSON array. \"\n",
        "            \"Ensure the generated emails are plausible and distinct from the references.\"\n",
        "            \"Always wrap your JSON output in triple backticks with 'json' language specifier, e.g., ```json [...]```.\"\n",
        "        )\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "        Generate exactly {num_to_generate} new and distinct corporate email snippets.\n",
        "        Each snippet must be categorized as '{target_label_name}'.\n",
        "\n",
        "        Here are some existing examples for reference, categorized as '{target_label_name}'.\n",
        "        These are provided as a JSON array of objects, with 'text' and 'label' keys:\n",
        "        ```json\n",
        "        [\n",
        "        {reference_str}\n",
        "        ]\n",
        "        ```\n",
        "\n",
        "        Produce your output as a single JSON array, directly.\n",
        "        Each element in the array must be a JSON object with two keys:\n",
        "        1. \"text\": The generated email snippet.\n",
        "        2. \"label\": The category, which must be '{target_label_name}'.\n",
        "\n",
        "        Output ONLY the JSON array, and wrap it in triple backticks with the 'json' language specifier (e.g., ```json [...]``` ).\n",
        "        For example:\n",
        "        ```json\n",
        "        [\n",
        "          {{\"text\": \"Generated email snippet 1\", \"label\": \"{target_label_name}\"}},\n",
        "          {{\"text\": \"Generated email snippet 2\", \"label\": \"{target_label_name}\"}}\n",
        "        ]\n",
        "        ```\n",
        "        \"\"\"\n",
        "\n",
        "        json_output_str = None # Initialize json_output_str\n",
        "\n",
        "        try:\n",
        "            chat_completion = self.client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                model=self.groq_model,\n",
        "                temperature=0.8,\n",
        "                max_tokens=int(num_to_generate * 100)\n",
        "            )\n",
        "\n",
        "            # Extract content and try to clean it\n",
        "            json_output_str = chat_completion.choices[0].message.content.strip()\n",
        "            # print(json_output_str)\n",
        "\n",
        "            if json_output_str.startswith(\"```\") :\n",
        "                cleaned_json_str = json_output_str[len(\"```\"):].strip()\n",
        "                print('check 0')\n",
        "            if cleaned_json_str.startswith(\"json\") :\n",
        "                cleaned_json_str = cleaned_json_str[len(\"json\"):].strip()\n",
        "                print('check 1')\n",
        "            if cleaned_json_str.endswith(\"```\") :\n",
        "                cleaned_json_str = cleaned_json_str[:-len(\"```\")].strip()\n",
        "                print('check 2')\n",
        "            else:\n",
        "                # If it didn't use backticks, try parsing directly as the model might still try to output raw JSON\n",
        "                # cleaned_json_str = json_output_str\n",
        "                print('check 3')\n",
        "            # print('out', cleaned_json_str)\n",
        "\n",
        "            parsed_raw = None\n",
        "            try:\n",
        "                parsed_raw = json.loads(cleaned_json_str)\n",
        "            except json.JSONDecodeError as de:\n",
        "                print(f\"Secondary JSON parsing failed: {de}\")\n",
        "                raise json.JSONDecodeError(\"Failed to parse JSON after initial cleaning attempts.\", cleaned_json_str, 0)\n",
        "\n",
        "\n",
        "            # After parsing (which might have been a dict or a list)\n",
        "            generated_data_list = []\n",
        "            if isinstance(parsed_raw, dict) and 'data' in parsed_raw and isinstance(parsed_raw['data'], list):\n",
        "                generated_data_list = parsed_raw['data']\n",
        "                print(\"Note: LLM output was wrapped in 'data' key, extracted successfully.\")\n",
        "            elif isinstance(parsed_raw, list):\n",
        "                generated_data_list = parsed_raw\n",
        "            else:\n",
        "                raise ValueError(f\"LLM output is not a direct JSON array or wrapped in a 'data' key as expected. Type: {type(parsed_raw)}\")\n",
        "\n",
        "            # Validate the structure of each item\n",
        "            validated_examples = []\n",
        "            for item in generated_data_list:\n",
        "                if isinstance(item, dict) and 'text' in item and 'label' in item:\n",
        "                    if item['label'] == target_label_name:\n",
        "                        validated_examples.append(item)\n",
        "                    else:\n",
        "                        print(f\"Warning: Generated label '{item['label']}' does not match target '{target_label_name}'. Skipping.\")\n",
        "                else:\n",
        "                    print(f\"Warning: Invalid item structure received from LLM: {item}. Skipping.\")\n",
        "            return validated_examples\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error parsing JSON from Groq API for {target_label_name}: {e}\")\n",
        "            print(f\"Raw LLM output (for debugging): {json_output_str}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling Groq API or processing response for {target_label_name}: {e}\")\n",
        "            print(f\"Raw LLM output (if available): {json_output_str}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "# Instantiate the synthetic data generator\n",
        "generator = SyntheticTextGenerator(id_to_label)\n",
        "\n",
        "label_counts = df_cleaned['label'].value_counts()\n",
        "under_represented_label_names = label_counts[label_counts < 15].index.tolist()\n",
        "\n",
        "augmented_data = []\n",
        "temp = ['Financial Report']\n",
        "\n",
        "for label_name in under_represented_label_names:\n",
        "    print(f\"\\nAugmenting label: '{label_name}'\")\n",
        "    current_count = label_counts[label_name]\n",
        "\n",
        "    # Get existing examples for reference\n",
        "    reference_df = df_cleaned[df_cleaned['label'] == label_name]\n",
        "\n",
        "    num_references = min(5, len(reference_df)) # Use up to 5 references\n",
        "    reference_texts = random.sample(reference_df['text'].tolist(), num_references)\n",
        "\n",
        "    # Determine how many new examples are needed to reach TARGET\n",
        "    num_to_generate = 15 - current_count\n",
        "\n",
        "    print(f\"  Need to generate {num_to_generate} new examples for '{label_name}'.\")\n",
        "\n",
        "    label_id = label_to_id[label_name]\n",
        "    new_generated_examples = generator.generate_similar_texts(reference_texts, num_to_generate, label_id)\n",
        "\n",
        "    augmented_data.extend(new_generated_examples)\n",
        "    print(f\"  Generated {len(new_generated_examples)} new examples for '{label_name}'.\")\n",
        "\n",
        "# Now, 'augmented_data' contains a list of dictionaries, each with 'text' and 'label'\n",
        "print(f\"\\n--- Overall Generated Synthetic Data ---\")\n",
        "print(f\"Total new synthetic examples generated across all under-represented labels: {len(augmented_data)}\")\n",
        "if augmented_data:\n",
        "    # Print a sample of the generated data to verify format\n",
        "    print(\"\\nSample of generated data (first 3):\")\n",
        "    for i, ex in enumerate(augmented_data[:3]):\n",
        "        print(f\"  {i+1}. Text: '{ex['text'][:70]}...', Label: '{ex['label']}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT-_MJNNrJZO"
      },
      "outputs": [],
      "source": [
        "if augmented_data:\n",
        "    augmented_df = pd.DataFrame(augmented_data)\n",
        "    df_augmented = pd.concat([df_cleaned, augmented_df], ignore_index=True)\n",
        "    print(f\"Total rows after augmentation: {len(df_augmented)}\")\n",
        "    df_cleaned = df_augmented.copy()\n",
        "else:\n",
        "    print(\"\\nNo data augmentation performed as no under-represented labels were found or generation failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1l2-H74ghTi"
      },
      "outputs": [],
      "source": [
        "def map_labels_to_ids_for_augmented(row):\n",
        "    # Check if the 'label' in the row is a string before attempting to map\n",
        "    if isinstance(row['label'], str):\n",
        "        row['label'] = label_to_id[row['label']]\n",
        "    return row\n",
        "\n",
        "# Convert df_cleaned (now potentially df_augmented) to a Hugging Face Dataset\n",
        "# Use .apply(..., axis=1) to apply the function row-wise\n",
        "df_cleaned = df_cleaned.apply(map_labels_to_ids_for_augmented, axis=1)\n",
        "# df_cleaned.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d_W-M_zlCGa"
      },
      "outputs": [],
      "source": [
        "# Now, convert the DataFrame to a Hugging Face Dataset\n",
        "print(\"Final Dataset object created successfully:\")\n",
        "print(full_labeled_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql6zoHV1qHXQ"
      },
      "outputs": [],
      "source": [
        "full_labeled_ds=full_labeled_ds.remove_columns([\"idx\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUNl5bhTPlnA"
      },
      "outputs": [],
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Move model to GPU if available\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuGferVTPpIs"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    predictions = np.argmax(p.predictions, axis=1)\n",
        "    labels = np.array(p.label_ids).flatten()\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n",
        "    precision_macro = precision_score(labels, predictions, average='macro', zero_division=0)\n",
        "    recall_macro = recall_score(labels, predictions, average='macro', zero_division=0)\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_weighted': f1_macro,\n",
        "        'precision_weighted': precision_macro,\n",
        "        'recall_weighted': recall_macro,\n",
        "    }\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK2DHmRRPinS"
      },
      "outputs": [],
      "source": [
        "train_val_test_split = full_labeled_ds.train_test_split(test_size=0.1, seed=42)\n",
        "train_val_ds = train_val_test_split[\"train\"]\n",
        "test_ds_final = train_val_test_split[\"test\"] # This is our final, labeled test set\n",
        "\n",
        "train_val_split = train_val_ds.train_test_split(test_size=0.11, seed=42)\n",
        "train_ds_split = train_val_split[\"train\"]\n",
        "eval_ds_split = train_val_split[\"test\"] # This is our validation set\n",
        "\n",
        "print(f\"\\nDataset Splits:\")\n",
        "print(f\"  Training samples: {len(train_ds_split)}\")\n",
        "print(f\"  Validation samples: {len(eval_ds_split)}\")\n",
        "print(f\"  Final Test samples: {len(test_ds_final)}\")\n",
        "\n",
        "print(\"\\nTrain Dataset Split Structure:\", train_ds_split)\n",
        "print(\"Validation Dataset Split Structure:\", eval_ds_split)\n",
        "print(\"Final Test Dataset Structure:\", test_ds_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns6ojsjrPnP6"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train_ds = train_ds_split.map(tokenize_function, batched=True)\n",
        "tokenized_eval_ds = eval_ds_split.map(tokenize_function, batched=True)\n",
        "tokenized_test_ds_final = test_ds_final.map(tokenize_function, batched=True) # Tokenize final test set\n",
        "\n",
        "# Remove original text and idx columns as they are no longer needed for training/evaluation\n",
        "tokenized_train_ds = tokenized_train_ds.remove_columns([\"text\"])\n",
        "tokenized_eval_ds = tokenized_eval_ds.remove_columns([\"text\"])\n",
        "tokenized_test_ds_final = tokenized_test_ds_final.remove_columns([\"text\"]) # Keep label for final test\n",
        "\n",
        "print(tokenized_train_ds)\n",
        "# print(len(tokenized_eval_ds))\n",
        "# print(len(tokenized_test_ds_final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpJR1RoSPuLe"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_mail_category\", # Directory for logs and checkpoints\n",
        "    num_train_epochs=10,                  # Number of training epochs\n",
        "    per_device_train_batch_size=16,      # Batch size for training\n",
        "    per_device_eval_batch_size=16,       # Batch size for evaluation\n",
        "    warmup_steps=10,                     # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                   # Strength of weight decay\n",
        "    logging_dir=\"./logs_mail_category\",  # Directory for storing logs\n",
        "    logging_strategy=\"epoch\",            # Log metrics at the end of each epoch\n",
        "    save_strategy=\"epoch\",               # Save model at the end of each epoch\n",
        "    eval_strategy=\"epoch\",               # Evaluate at the end of each epoch\n",
        "    load_best_model_at_end=True,         # Load the best model at the end of training\n",
        "    metric_for_best_model=\"f1_weighted\", # Metric to use to compare models\n",
        "    report_to=\"none\",                    # Don't report to any online services\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_ds,\n",
        "    eval_dataset=tokenized_eval_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Start training!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n--- 7. Evaluation Metrics on Validation Set ---\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\\nEvaluation Metrics on Validation Set (used during training):\")\n",
        "eval_df = pd.DataFrame([eval_results]).transpose()\n",
        "eval_df.columns = ['Value']\n",
        "print(tabulate(eval_df, headers='keys', tablefmt='grid', floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfBw_ukNLcQA"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Saving the Fine-tuned Model ---\")\n",
        "save_path = \"./mail_category\"\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"\\nModel and tokenizer saved to: {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPybqOycv7yOxSGXFp3OZmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}