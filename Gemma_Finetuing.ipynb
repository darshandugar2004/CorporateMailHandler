{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshandugar2004/CorporateMailHandler/blob/main/Gemma_Finetuing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL0M_6-AR-GA"
      },
      "source": [
        "## Artificial Data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzks7vQPUn-q"
      },
      "outputs": [],
      "source": [
        "# Install required libraries for fine-tuning\n",
        "!pip install -q -U transformers datasets accelerate peft trl\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artifiical Data Generation"
      ],
      "metadata": {
        "id": "--FHpydp3IZu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNJ3q5AiPCdG"
      },
      "outputs": [],
      "source": [
        "# Block 1: Generate an Enhanced and Diverse Training Dataset\n",
        "\n",
        "import json\n",
        "\n",
        "# ==============================================================================\n",
        "# DATASET ENHANCEMENT NOTES:\n",
        "# 1. Quality Check: Manually review each \"completion\" for perfect grammar and tone.\n",
        "# 2. Further Expansion: To generate more high-quality data, consider using a\n",
        "#    powerful LLM with a prompt like:\n",
        "#    \"Generate 10 diverse training examples for a corporate assistant LLM with the\n",
        "#     intent 'Sustainability Initiative'. Include successes, failures, and requests\n",
        "#     for clarification. Format as a JSON object with 'details' and 'completion' keys.\"\n",
        "# ==============================================================================\n",
        "\n",
        "scenarios = {\n",
        "    \"Merger Announcement\": [\n",
        "        \"Details: Verification API: SUCCESS. Ticket #T5821 raised for SM approval.\",\n",
        "        \"Details: Verification API: FAILED - Inconsistent company registration number. Ticket #T522 raised for manual review.\",\n",
        "        \"Details: Verification API: PARTIAL SUCCESS. Director names verified, financial statements pending upload. Ticket #T5823 for follow-up.\",\n",
        "        \"Details: Verification API: ERROR - Service timed out. Ticket #T5824 raised with IT. Will re-attempt verification.\",\n",
        "        \"Details: Senior Manager approval received for Ticket #T5821. Notifying the legal team to proceed.\",\n",
        "        # --- NEW DIVERSITY EXAMPLE (URGENCY) ---\n",
        "        \"Details: URGENT - Verification API SUCCESS. Ticket #T5825 raised for IMMEDIATE SM approval due to market sensitivity.\",\n",
        "    ],\n",
        "    \"Sustainability Initiative\": [\n",
        "        \"Details: RAG system extracted the following from Q3 report: 'Carbon emissions reduced by 15% year-over-year, exceeding the 10% target.'\",\n",
        "        \"Details: User asked for water usage metrics. RAG system found no specific data for 'water usage' in the latest report.\",\n",
        "        \"Details: User inquired about waste reduction. RAG found: 'Our Zero Waste to Landfill initiative successfully diverted 95% of waste in the past year.'\",\n",
        "        \"Details: User asked for data on renewable energy. RAG found: 'As of Q3 2025, 45% of our global energy consumption is from renewable sources.'\",\n",
        "        \"Details: User asked for volunteering hours and packaging sources. RAG found volunteering data: '10,000+ employee volunteer hours contributed.' No data found for 'packaging sources.'\",\n",
        "        # --- NEW DIVERSITY EXAMPLE (AMBIGUITY) ---\n",
        "        \"Details: User sent a vague email: 'send sustainability info'. RAG system cannot proceed without more specific keywords.\",\n",
        "    ]\n",
        "}\n",
        "\n",
        "completions = {\n",
        "    \"Merger Announcement\": [\n",
        "        \"Dear Team,\\n\\nThis is to confirm that the preliminary details for the upcoming merger have been successfully verified. A ticket (#T5821) has been raised for the Senior Manager's final approval.\\n\\nFurther updates will be shared post-approval.\\n\\nBest regards,\\nCorporate Assistant\",\n",
        "        \"Dear Team,\\n\\nAn issue was encountered while verifying the details for the upcoming merger. The company registration number was found to be inconsistent. A ticket (#T5822) has been created for the corporate governance team to conduct a manual review.\\n\\nWe will provide an update once the review is complete.\\n\\nBest regards,\\nCorporate Assistant\",\n",
        "        \"Dear Team,\\n\\nA partial verification was completed for the upcoming merger. While director names have been confirmed, the required financial statements are pending upload. A follow-up ticket (#T5823) has been created to track this item.\\n\\nRegards,\\nCorporate Assistant\",\n",
        "        \"Dear Team,\\n\\nWe encountered a temporary system issue while attempting to verify the merger details, as the verification service timed out. The IT department has been notified via ticket #T5824. The verification process will be re-attempted shortly.\\n\\nThank you for your patience.\\n\\nBest regards,\\nCorporate Assistant\",\n",
        "        \"Dear Team,\\n\\nThis is an update regarding the proposed merger. The Senior Manager has formally approved the initial verification (Ticket #T5821). The legal team has been notified and will now proceed with the subsequent steps.\\n\\nFurther communication will follow from the legal department.\\n\\nBest regards,\\nCorporate Assistant\",\n",
        "        # --- NEW DIVERSITY COMPLETION (URGENCY) ---\n",
        "        \"URGENT ACTION REQUIRED\\n\\nDear Team,\\n\\nThis is to confirm that the preliminary details for the time-sensitive merger have been successfully verified. An urgent ticket (#T5825) has been raised for IMMEDIATE Senior Manager approval.\\n\\nYour prompt attention is required.\\n\\nBest regards,\\nCorporate Assistant\",\n",
        "    ],\n",
        "    \"Sustainability Initiative\": [\n",
        "        \"Dear Stakeholder,\\n\\nIn response to your query regarding our sustainability efforts, our Q3 report confirms a significant achievement: carbon emissions were reduced by 15% year-over-year, which surpasses our stated goal of 10%.\\n\\nThank you for your interest in our initiatives.\\n\\nSincerely,\\nCorporate Assistant\",\n",
        "        \"Dear Stakeholder,\\n\\nThank you for your inquiry. While our latest sustainability report details our progress in areas like carbon emissions, it does not contain specific metrics on water usage.\\n\\nYour query has been forwarded to the sustainability department for more detailed information.\\n\\nSincerely,\\nCorporate Assistant\",\n",
        "        \"Dear Stakeholder,\\n\\nRegarding your inquiry on waste reduction, our latest report confirms that our 'Zero Waste to Landfill' initiative has been highly effective, successfully diverting 95% of our manufacturing waste from landfills in the past year.\\n\\nWe appreciate your interest in our environmental programs.\\n\\nSincerely,\\nCorporate Assistant\",\n",
        "        \"Dear Stakeholder,\\n\\nIn response to your query about renewable energy, our data from Q3 2025 shows that 45% of our global energy consumption is now sourced from renewable providers, primarily solar and wind.\\n\\nThank you for your engagement on this important topic.\\n\\nSincerely,\\nCorporate Assistant\",\n",
        "        \"Dear Stakeholder,\\n\\nThank you for your inquiry. In response to your questions:\\n- Our employees have contributed over 10,000 volunteer hours to community projects this year.\\n- Our current sustainability report does not contain specific data on packaging material sources.\\n\\nYour query on packaging has been forwarded to the supply chain department for further details.\\n\\nSincerely,\\nCorporate Assistant\",\n",
        "        # --- NEW DIVERSITY COMPLETION (AMBIGUITY) ---\n",
        "        \"Dear Stakeholder,\\n\\nThank you for your interest in our sustainability initiatives. To provide you with the most relevant information, could you please specify which area you are interested in (e.g., carbon emissions, waste reduction, renewable energy)?\\n\\nWe look forward to providing you with the data you need.\\n\\nSincerely,\\nCorporate Assistant\",\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "# Create the formatted dataset (No changes to this logic)\n",
        "dataset = []\n",
        "for intent, details_list in scenarios.items():\n",
        "    # Adding a check to prevent errors if completions are missing\n",
        "    if intent in completions:\n",
        "        for i, details in enumerate(details_list):\n",
        "            prompt = f\"As a corporate assistant, write a formal email based on the following intent and details. Intent: {intent}. {details}\"\n",
        "            completion = completions[intent][i]\n",
        "            formatted_text = f\"<s>[INST] {prompt} [/INST] {completion}</s>\"\n",
        "            dataset.append({\"text\": formatted_text})\n",
        "\n",
        "# Save the dataset to a JSONL file\n",
        "output_file = \"style_dataset.jsonl\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    for entry in dataset:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"Enhanced dataset with {len(dataset)} examples created and saved to '{output_file}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsuZ1UenQ-VP"
      },
      "outputs": [],
      "source": [
        "print(f\"Dataset with {len(dataset)} examples created and saved to '{output_file}'\")\n",
        "print(\"\\n--- Sample Entry ---\")\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwuKyBQVSX5n"
      },
      "source": [
        "## Finetune LLAMA for email generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl_-VASAJtQN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset_file = \"style_dataset.jsonl\"\n",
        "# --- 4. Load Dataset ---\n",
        "dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fmiflm6834ra",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install trl\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeG64qkd485w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# --- 3. Configuration ---\n",
        "# CHANGED: Model ID is now Gemma-2B\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "dataset_file = \"style_dataset.jsonl\"\n",
        "# CHANGED: New output directory for the new model\n",
        "output_dir = \"./fine_tuned_gemma_2b_adapters\"\n",
        "\n",
        "# --- 4. Load Dataset ---\n",
        "dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njB3GU-t7AmX"
      },
      "outputs": [],
      "source": [
        "# --- 4. Define Quantization Configuration ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    # bnb_4bit_quant_type=\"nf4\",\n",
        "    # bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# --- 5. Load Model and Tokenizer ---\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    # quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    # use_auth_token=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare a Test Prompt ---\n",
        "prompt = \"As a corporate assistant, write a formal email based on the following intent and details. Intent: Merger Announcement. Details: Verification API: FAILED - Inconsistent company registration number. Ticket #T9999 raised for manual review.\"\n",
        "# Use the Gemma-specific prompt format\n",
        "formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "# --- Generate the Response ---\n",
        "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "completion = response_text.split(\"<start_of_turn>model\\n\")[-1]\n",
        "\n",
        "print(\"--- RESPONSE FROM BASE GEMMA-2B MODEL (BEFORE TRAINING) ---\")\n",
        "print(completion)"
      ],
      "metadata": {
        "id": "RSZisGhhT8f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "B8wlDdhlT46D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Configure LoRA ---\n",
        "# The target modules are the same for Gemma\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "# No need for `prepare_model_for_kbit_training` in recent PEFT versions\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "Ov6PGyexCNLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ntOQn1GdMY"
      },
      "outputs": [],
      "source": [
        "# --- 7. Set Up Training ---\n",
        "# NOTE: Increased batch size as Gemma-2B is smaller and uses less memory\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4,  # Increased from 2\n",
        "    gradient_accumulation_steps=2, # Decreased to keep effective batch size (4*2=8)\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config,\n",
        "    # tokenizer=tokenizer,\n",
        "    # max_seq_length=1024\n",
        ")\n",
        "\n",
        "# --- 8. Start Training ---\n",
        "print(\"Starting Gemma-2B fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")\n",
        "\n",
        "# --- 9. Save the Final Adapters ---\n",
        "trainer.save_model(output_dir)\n",
        "print(f\"Gemma-2B model adapters saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ðŸš€ PART 3: RUNNING A TEST INFERENCE\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Running test inference ---\")\n",
        "\n",
        "# --- Step 3.1: Prepare a test prompt ---\n",
        "# This prompt must follow the EXACT same format as your training data.\n",
        "test_intent = \"Merger Announcement\"\n",
        "test_details = \"Details: Verification API: FAILED - Inconsistent company registration number. Ticket #T9999 raised for manual review.\"\n",
        "\n",
        "prompt = f\"As a corporate assistant, write a formal email based on the following intent and details. Intent: {test_intent}. {test_details}\"\n",
        "formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "# --- Step 3.2: Generate the response ---\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "\n",
        "# Generate the output with optimized parameters\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.2, # Low temperature for professional, predictable output\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "# --- Step 3.3: Decode and print the result ---\n",
        "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Clean up the output to only show the model's completion\n",
        "completion = response_text.split(\"<start_of_turn>model\\n\")[-1]\n",
        "\n",
        "print(f\"\\nPROMPT:\\n{prompt}\")\n",
        "print(\"-\" * 20)\n",
        "print(f\"MODEL RESPONSE:\\n{completion}\")"
      ],
      "metadata": {
        "id": "03ROk0iECPzX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}